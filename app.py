import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import networkx as nx
import matplotlib.pyplot as plt
import json
import os

# –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
DATA_FILE = "chat_memory.json"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
def load_chat_history():
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, "r") as f:
            return json.load(f)
    return []

def save_chat_history(data):
    with open(DATA_FILE, "w") as f:
        json.dump(data, f)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title("ü§ñ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å")

# –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ GPT-2
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2", cache_dir="./cache")
    model = AutoModelForCausalLM.from_pretrained("distilgpt2", cache_dir="./cache")
    return tokenizer, model

tokenizer, model = load_model()

# **–ù–ê–°–¢–†–û–ô–ö–ò –ù–ï–ô–†–û–°–ï–¢–ò**
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
layers = st.sidebar.text_input("–°–ª–æ–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10,20,10):", "10,20,10")
layers = list(map(int, layers.split(",")))

# –í—ã–±–æ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
activation = st.sidebar.selectbox("–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:", ["relu", "sigmoid", "tanh"])

# –§—É–Ω–∫—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ—Ç–∏
def visualize_nn(layers):
    G = nx.DiGraph()
    
    for i, neurons in enumerate(layers):
        for j in range(neurons):
            G.add_node(f"L{i}_N{j}", layer=i)
        if i > 0:
            for prev in range(layers[i-1]):
                for curr in range(neurons):
                    G.add_edge(f"L{i-1}_N{prev}", f"L{i}_N{curr}")

    pos = nx.multipartite_layout(G, subset_key="layer")
    plt.figure(figsize=(10, 6))
    nx.draw(G, pos, with_labels=False, node_size=300, edge_color='gray')
    plt.title(f"üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: {activation})")
    st.pyplot(plt)

# –ö–Ω–æ–ø–∫–∞ "–û–±–Ω–æ–≤–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å"
if st.sidebar.button("–û–±–Ω–æ–≤–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å"):
    visualize_nn(layers)
    st.sidebar.write("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

# **–ß–ê–¢-–ë–û–¢ –° –ò–°–¢–û–†–ò–ï–ô**
st.subheader("üí¨ –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
chat_history = load_chat_history()

# –ü–æ–ª–µ –≤–≤–æ–¥–∞
question = st.text_input("")

if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    if not question.strip():
        st.warning("‚ùå –í–æ–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º!")
    else:
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å)
        history_text = "\n".join([f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {entry['question']}\n–ë–æ—Ç: {entry['answer']}" for entry in chat_history[-3:]])
        input_text = f"{history_text}\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}\n–ë–æ—Ç:"
        input_text = input_text[-500:]  # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã
        inputs = tokenizer.encode(input_text, return_tensors="pt")
        attention_mask = torch.ones_like(inputs)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º `max_length` –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
        max_input_length = inputs.shape[1]
        max_response_length = min(50, model.config.max_length - max_input_length)

        try:
            response_ids = model.generate(
                inputs, 
                max_length=max_input_length + max_response_length,  
                pad_token_id=tokenizer.eos_token_id,
                attention_mask=attention_mask
            )
            response = tokenizer.decode(response_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)
        except ValueError:
            response = "‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏! –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–Ω–∞—á–µ."

        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π ‚Äî –¥–∞—ë–º –∑–∞–≥–ª—É—à–∫—É
        if not response.strip():
            response = "–Ø –ø–æ–∫–∞ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ —Å–∫–∞–∑–∞—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–Ω–∞—á–µ!"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        chat_history.append({"question": question, "answer": response})
        save_chat_history(chat_history)

        st.write(f"ü§ñ **–û—Ç–≤–µ—Ç:** {response}")

# –í–´–í–û–î–ò–ú –ò–°–¢–û–†–ò–Æ –ß–ê–¢–ê
st.subheader("üìú –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:")
for entry in chat_history[-3:]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è
    st.write(f"**–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:** {entry['question']}")
    st.write(f"**–ë–æ—Ç:** {entry['answer']}")
    st.write("---")

# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –°–¢–†–£–ö–¢–£–†–´ –ù–ï–ô–†–û–°–ï–¢–ò
st.subheader("üï∏Ô∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
visualize_nn(layers)
