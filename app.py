import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import networkx as nx
import matplotlib.pyplot as plt

# –ó–ê–ì–û–õ–û–í–û–ö
st.title("ü§ñ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤")

# –§–£–ù–ö–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò GPT-2
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2", cache_dir="./cache")
    model = AutoModelForCausalLM.from_pretrained("distilgpt2", cache_dir="./cache")
    return tokenizer, model

tokenizer, model = load_model()

# üõ†Ô∏è **–ù–ê–°–¢–†–û–ô–ö–ò –ù–ï–ô–†–û–°–ï–¢–ò**
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
layers = st.sidebar.text_input("–°–ª–æ–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10,20,10):", "10,20,10")
layers = list(map(int, layers.split(",")))
activation = st.sidebar.selectbox("–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:", ["relu", "sigmoid", "tanh"])

# –§–£–ù–ö–¶–ò–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ù–ï–ô–†–û–°–ï–¢–ò
def visualize_nn(layers):
    G = nx.DiGraph()
    
    for i, neurons in enumerate(layers):
        for j in range(neurons):
            G.add_node(f"L{i}_N{j}", layer=i)
        if i > 0:
            for prev in range(layers[i-1]):
                for curr in range(neurons):
                    G.add_edge(f"L{i-1}_N{prev}", f"L{i}_N{curr}")

    pos = nx.multipartite_layout(G, subset_key="layer")
    plt.figure(figsize=(10, 6))
    nx.draw(G, pos, with_labels=False, node_size=300, edge_color='gray')
    plt.title("üß† –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
    st.pyplot(plt)

# –ö–ù–û–ü–ö–ê "–û–ë–ù–û–í–ò–¢–¨ –ù–ï–ô–†–û–°–ï–¢–¨"
if st.sidebar.button("–û–±–Ω–æ–≤–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å"):
    visualize_nn(layers)
    st.sidebar.write("‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

# üìù **–ß–ê–¢-–ë–û–¢**
st.subheader("üí¨ –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")
question = st.text_input("")

if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    inputs = tokenizer.encode(question, return_tensors="pt")
    response_ids = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(response_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)

    st.write(f"ü§ñ **–û—Ç–≤–µ—Ç:** {response}")

# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –°–¢–†–£–ö–¢–£–†–´ –ù–ï–ô–†–û–°–ï–¢–ò
st.subheader("üï∏Ô∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
visualize_nn(layers)
