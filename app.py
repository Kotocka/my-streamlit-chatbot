import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import networkx as nx
import matplotlib.pyplot as plt
import json
import os

# –§–∞–π–ª, –≥–¥–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–∞–º—è—Ç—å —á–∞—Ç–∞)
DATA_FILE = "chat_memory.json"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏/—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–∞—Ç–∞
def load_data():
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, "r") as f:
            return json.load(f)
    return []

def save_data(data):
    with open(DATA_FILE, "w") as f:
        json.dump(data, f)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
def visualize_nn(model):
    G = nx.DiGraph()

    layers = []
    for i, (name, param) in enumerate(model.named_parameters()):
        if "weight" in name:
            layers.append(param.shape[0])  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–ª–æ–µ

    # –°–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–æ–≤ –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
    for i, neurons in enumerate(layers):
        for j in range(neurons):
            G.add_node(f"L{i}_N{j}", layer=i)
        if i > 0:
            for prev in range(layers[i-1]):
                for curr in range(neurons):
                    G.add_edge(f"L{i-1}_N{prev}", f"L{i}_N{curr}")

    pos = nx.multipartite_layout(G, subset_key="layer")
    plt.figure(figsize=(10, 6))
    nx.draw(G, pos, with_labels=False, node_size=300, edge_color='gray')
    plt.title("üß† –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏")
    st.pyplot(plt)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å GPT-2
@st.cache_resource
def load_gpt2():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
    model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
    return tokenizer, model

tokenizer, model = load_gpt2()

st.title("ü§ñ –î–∏–∞–ª–æ–≥–æ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
chat_history = load_data()

# –ü–æ–ª–µ –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

# –ö–Ω–æ–ø–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏
if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏–∏
    existing_answer = next((entry["answer"] for entry in chat_history if entry["question"] == question), None)

    if existing_answer:
        response = existing_answer  # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —É–∂–µ –±—ã–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –æ—Ç–≤–µ—Ç
    else:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é GPT-2
        inputs = tokenizer.encode(question + tokenizer.eos_token, return_tensors="pt")
        response_ids = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)
        response = tokenizer.decode(response_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)

        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç
        chat_history.append({"question": question, "answer": response})
        save_data(chat_history)

    st.write(f"üß† **–û—Ç–≤–µ—Ç:** {response}")

# –í—ã–≤–æ–¥–∏–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
st.subheader("üìù –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:")
for entry in chat_history[-5:]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
    st.write(f"**Q:** {entry['question']}")
    st.write(f"**A:** {entry['answer']}")
    st.write("---")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
st.subheader("üï∏Ô∏è –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:")
visualize_nn(model)
