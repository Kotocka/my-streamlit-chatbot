import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

st.title("ü§ñ –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å")

# –ö—ç—à–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –µ—ë –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("distilgpt2", cache_dir="./cache")
    model = AutoModelForCausalLM.from_pretrained("distilgpt2", cache_dir="./cache")
    return tokenizer, model

tokenizer, model = load_model()

# –ü–æ–ª–µ –≤–≤–æ–¥–∞
question = st.text_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å:")

if st.button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å"):
    inputs = tokenizer.encode(question, return_tensors="pt")
    response_ids = model.generate(inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(response_ids[:, inputs.shape[-1]:][0], skip_special_tokens=True)
    
    st.write(f"ü§ñ **–û—Ç–≤–µ—Ç:** {response}")
